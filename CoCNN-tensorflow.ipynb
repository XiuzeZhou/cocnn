{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Dataset and evaluation protocols reused from\n",
    "# https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "from Dataset import Dataset\n",
    "from evaluate import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data sequence from user-item matrix\n",
    "def generate_instances(train_mat, positive_size=1, negative_time=4, is_sparse=False):\n",
    "    data = []\n",
    "    users_num,items_num = train_mat.shape\n",
    "    \n",
    "    if is_sparse:\n",
    "        indptr = train_mat.indptr\n",
    "        indices = train_mat.indices\n",
    "    for u in range(users_num):\n",
    "        if is_sparse:\n",
    "            rated_items = indices[indptr[u]:indptr[u+1]] #用户u中有评分项的id\n",
    "        else:\n",
    "            rated_items = np.where(train_mat[u,:]>0)[0]\n",
    "        \n",
    "        for item0 in rated_items:\n",
    "            for item1 in np.random.choice(rated_items, size=positive_size):\n",
    "                data.append([u,item0,item1,1.])\n",
    "            for _ in range(positive_size*negative_time):\n",
    "                item1 = np.random.randint(items_num) # no matter item1 is positive or negtive\n",
    "                item2 = np.random.randint(items_num)\n",
    "                while item2 in rated_items:\n",
    "                    item2 = np.random.randint(items_num)\n",
    "                data.append([u,item2,item1,0.])\n",
    "    return data\n",
    "\n",
    "# read data sequence from file generated by generate_instances function\n",
    "def read_list(file_dir):\n",
    "    data = []\n",
    "    with open(file_dir, \"r\", encoding='utf-8') as f:\n",
    "        data = [[int(line.split()[0]), int(line.split()[1]), int(line.split()[2]), float(line.split()[3])] for line in f.readlines()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "def evaluate(model, test_ratings, test_negatives, K=10):\n",
    "    \"\"\"Helper that calls evaluate from the NCF libraries.\"\"\"\n",
    "    (hits, ndcgs) = evaluate_model(model, test_ratings, test_negatives, K=K, num_thread=1)\n",
    "    return np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "\n",
    "\n",
    "def get_similar_items(item_mat, idx, topk=5):\n",
    "    m,k = item_mat.shape\n",
    "    target_item = item_mat[idx,:]\n",
    "    target_mat = np.reshape(np.tile(target_item,m),(-1,k))\n",
    "    sim = [np.dot(target_mat[i], item_mat[i])/(np.linalg.norm(target_mat[i])*np.linalg.norm(item_mat[i])) \n",
    "           for i in range(m)] \n",
    "    sorted_items = np.argsort(-np.array(sim))\n",
    "    return sorted_items[:topk+1] # the most similar is itself\n",
    "\n",
    "def get_key(item_dict, value):\n",
    "    key = -1\n",
    "    for (k, v) in item_dict.items():\n",
    "        if v == value:\n",
    "            key = k\n",
    "    return key\n",
    "\n",
    "\n",
    "# read original records\n",
    "def get_item_dict(file_dir):\n",
    "    # output: \n",
    "    # N: the number of user;\n",
    "    # M: the number of item\n",
    "    # data: the list of rating information\n",
    "    user_ids_dict, rated_item_ids_dict = {},{}\n",
    "    N, M, u_idx, i_idx = 0,0,0,0 \n",
    "    data_rating = []\n",
    "    data_time = []\n",
    "    f = open(file_dir)\n",
    "    for line in f.readlines():\n",
    "        if '::' in line:\n",
    "            u, i, r = line.split('::')[:3]\n",
    "        elif ',' in line:\n",
    "            u, i, r = line.split(',')[:3]\n",
    "        else:\n",
    "            u, i, r = line.split()[:3]\n",
    "    \n",
    "        if u not in user_ids_dict:\n",
    "            user_ids_dict[u]=u_idx\n",
    "            u_idx+=1\n",
    "        if i not in rated_item_ids_dict:\n",
    "            rated_item_ids_dict[i]=i_idx\n",
    "            i_idx+=1\n",
    "        data_rating.append([user_ids_dict[u],rated_item_ids_dict[i],float(r)])\n",
    "    \n",
    "    f.close()\n",
    "    N = u_idx\n",
    "    M = i_idx\n",
    "\n",
    "    return rated_item_ids_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net():\n",
    "    def __init__(self,               \n",
    "                 users_num = None, #用户数\n",
    "                 items_num = None, #商品数\n",
    "                 batch_size = 1024, #batch大小\n",
    "                 embedding_size = 64, # 嵌入空间维度\n",
    "                 out_channels = 32,\n",
    "                 kernel_size = 3,\n",
    "                 learning_rate = 1e-3, #学习率\n",
    "                 lamda_regularizer = 1e-6,#正则项系数\n",
    "                 alpha = 1.0,\n",
    "                 seed = 2\n",
    "                ):\n",
    "        self.users_num = users_num\n",
    "        self.items_num = items_num\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lamda_regularizer = lamda_regularizer\n",
    "        self.alpha = alpha\n",
    "        self.seed = seed\n",
    "        self.padding = 0 # 不填充\n",
    "        self.stride = 1\n",
    "\n",
    "        # loss records\n",
    "        self.train_loss_records = []  \n",
    "        self.build_graph()   \n",
    "\n",
    "        \n",
    "    def build_graph(self):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            setup_seed(self.seed)\n",
    "            \n",
    "            # _________ input data _________\n",
    "            self.user_inputs = tf.placeholder(tf.int32, shape = [None, 1], name='user_inputs')\n",
    "            self.item_inputs = tf.placeholder(tf.int32, shape = [None, 2], name='item_inputs')\n",
    "            self.train_labels = tf.placeholder(tf.float32, shape = [None, 1], name='train_labels') \n",
    "            \n",
    "            # _________ variables _________\n",
    "            self.weights = self._initialize_weights()\n",
    "            \n",
    "            # _________ train _____________\n",
    "            self.y_uij, self.y_ui = self.inference(user_inputs=self.user_inputs, item_inputs=self.item_inputs)\n",
    "            self.loss_train = self.loss_function(true_labels=self.train_labels, \n",
    "                                                 y_uij=tf.reshape(self.y_uij,shape=[-1, 1]),\n",
    "                                                 y_ui=tf.reshape(self.y_ui,shape=[-1, 1]),\n",
    "                                                 lamda_regularizer=self.lamda_regularizer)\n",
    "            self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate,beta1=0.9, beta2=0.999, epsilon=1e-08).minimize(self.loss_train) \n",
    "\n",
    "            # _________ prediction _____________\n",
    "            self.predictions,_ = self.inference(user_inputs=self.user_inputs, item_inputs=self.item_inputs)\n",
    "        \n",
    "            #变量初始化 init\n",
    "            self.saver = tf.train.Saver() #  \n",
    "            init = tf.global_variables_initializer()\n",
    "            self.sess = self._init_session()\n",
    "            self.sess.run(init)\n",
    "    \n",
    "    \n",
    "    def _init_session(self):\n",
    "        # adaptively growing memory\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        return tf.Session(config=config)\n",
    "    \n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "\n",
    "        # -----embeddings------\n",
    "        all_weights['embedding_users'] = tf.Variable(tf.random_normal([self.users_num, self.embedding_size], 0, 0.1),name='embedding_users')\n",
    "        all_weights['embedding_items'] = tf.Variable(tf.random_normal([self.items_num, self.embedding_size], 0, 0.1),name='embedding_items') \n",
    "        \n",
    "        # ------CNN for ui------\n",
    "        all_weights['cnn_ui'] = tf.Variable(tf.random_normal(\n",
    "            [2, 2, 1, self.out_channels], 0, 0.1),name='cnn_ui')\n",
    "        all_weights['bias_ui'] = tf.Variable(tf.zeros([self.out_channels]), name='bias_ui')\n",
    "        self.out_size_ui = int(((self.embedding_size - 2 + 2 * self.padding)/self.stride + 1) * self.out_channels)\n",
    "        all_weights['linear_0'] = tf.Variable(tf.random_normal([self.out_size_ui, 1], 0, 0.1), name='linear_0')\n",
    "        all_weights['bias_0'] = tf.Variable(tf.zeros([1]), name='bias_0')\n",
    "        \n",
    "        \n",
    "        # ------CNN for uij------\n",
    "        # filter=[filter_height,filter_width,in_channels,out_channels]\n",
    "        all_weights['cnn_1'] = tf.Variable(tf.random_normal(\n",
    "            [self.kernel_size, self.kernel_size, 1, self.out_channels], 0, 0.1),name='cnn_1')\n",
    "        all_weights['bias_1'] = tf.Variable(tf.zeros([self.out_channels]), name='bias_1')\n",
    "        \n",
    "        input_size = self.embedding_size\n",
    "        if self.kernel_size == 2:\n",
    "            all_weights['cnn_2'] = tf.Variable(tf.random_normal(\n",
    "                [self.kernel_size, self.kernel_size, self.out_channels, self.out_channels], 0, 0.1),name='cnn_2')\n",
    "            all_weights['bias_2'] = tf.Variable(tf.zeros([self.out_channels]), name='bias_2')\n",
    "            input_size = (self.embedding_size - self.kernel_size + 2 * self.padding)/self.stride + 1\n",
    "            \n",
    "        self.out_size = int(((input_size - self.kernel_size + 2 * self.padding)/self.stride + 1) * self.out_channels)\n",
    "        all_weights['linear'] = tf.Variable(tf.random_normal([self.out_size, 1], 0, 0.1), name='linear')\n",
    "        all_weights['bias'] = tf.Variable(tf.zeros([1]), name='bias')\n",
    "\n",
    "        return all_weights\n",
    "        \n",
    "    \n",
    "    def train(self, data_sequence):\n",
    "        train_size = len(data_sequence)\n",
    "        \n",
    "        np.random.shuffle(data_sequence)\n",
    "        batch_size = self.batch_size\n",
    "        total_batch = math.ceil(train_size/batch_size)\n",
    "\n",
    "        for batch in range(total_batch):\n",
    "            start = (batch * batch_size)%train_size\n",
    "            end = min(start + batch_size, train_size)\n",
    "            data_array = np.array(data_sequence[start:end])\n",
    "\n",
    "            feed_dict = {self.user_inputs: np.reshape(data_array[:,0],(-1,1)), \n",
    "                         self.item_inputs: data_array[:,1:3],\n",
    "                         self.train_labels: np.reshape(data_array[:,-1],(-1,1))}  \n",
    "            loss, opt = self.sess.run([self.loss_train,self.train_op], feed_dict=feed_dict)\n",
    "            self.train_loss_records.append(loss)\n",
    "            \n",
    "        return self.train_loss_records\n",
    "\n",
    "        \n",
    "    # CNN for (u,i)\n",
    "    def net_ui(self, embed_users, embed_items):\n",
    "        connection = tf.reshape(tf.concat([embed_users, embed_items], 1), shape=[-1, 2, self.embedding_size, 1])\n",
    "        conv = tf.nn.conv2d(input=connection, filter=self.weights['cnn_ui'], strides=[1, self.stride, self.stride, 1], padding='VALID')\n",
    "        out = tf.nn.relu(tf.nn.bias_add(conv, self.weights['bias_ui']))\n",
    "        out = tf.reshape(out, [-1,self.out_size_ui])\n",
    "        y_ = tf.matmul(out, self.weights['linear_0']) + self.weights['bias_0']\n",
    "        return y_\n",
    "        \n",
    "        \n",
    "    # CNN for (u,i,j)\n",
    "    def net_uij(self, embed_users, embed_items0, embed_items1):\n",
    "        connection = tf.reshape(tf.concat([embed_items0, embed_users, embed_items1], 1), shape=[-1, 3, self.embedding_size, 1])\n",
    "        conv = tf.nn.conv2d(input=connection, filter=self.weights['cnn_1'], strides=[1, self.stride, self.stride, 1], padding='VALID')\n",
    "        # input shape: [ batch, in_height, in_width, in_channel ]\n",
    "        out = tf.nn.relu(tf.nn.bias_add(conv, self.weights['bias_1']))\n",
    "        \n",
    "        if self.kernel_size == 2:\n",
    "            conv = tf.nn.conv2d(input=out, filter=self.weights['cnn_2'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "            out = tf.nn.relu(tf.nn.bias_add(conv, self.weights['bias_2']))\n",
    "        \n",
    "        out = tf.reshape(out, [-1,self.out_size])\n",
    "        y_ = tf.matmul(out, self.weights['linear']) + self.weights['bias']\n",
    "        return y_\n",
    "    \n",
    "        \n",
    "    # 网络的前向传播\n",
    "    def inference(self, user_inputs, item_inputs):\n",
    "        embed_users = tf.reshape(tf.nn.embedding_lookup(self.weights['embedding_users'], user_inputs),\n",
    "                                 shape=[-1, self.embedding_size])\n",
    "        embed_items0 = tf.reshape(tf.nn.embedding_lookup(self.weights['embedding_items'], item_inputs[:,0]),\n",
    "                                 shape=[-1, self.embedding_size])\n",
    "        embed_items1 = tf.reshape(tf.nn.embedding_lookup(self.weights['embedding_items'], item_inputs[:,1]),\n",
    "                                 shape=[-1, self.embedding_size])\n",
    "        \n",
    "        y_uij = self.net_uij(embed_users, embed_items0, embed_items1)\n",
    "        y_ui = self.net_ui(embed_users, embed_items0)\n",
    "        \n",
    "        return y_uij, y_ui         \n",
    "        \n",
    "        \n",
    "    def loss_function(self, true_labels, y_uij, y_ui,lamda_regularizer=1e-6, loss_type='mse'):   \n",
    "        cost_ui = tf.losses.mean_squared_error(true_labels, y_ui)\n",
    "        cost_uij = tf.losses.mean_squared_error(true_labels, y_uij)\n",
    "        if loss_type == 'cross_entropy':\n",
    "            cost_uij = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=true_labels, logits=y_uij))\n",
    "            #cost = tf.reduce_mean(tf.square(1.+true_labels)*tf.square(true_labels-tf.sigmoid(predicted_labels)))\n",
    "            #mse = tf.losses.mean_squared_error(true_labels, tf.sigmoid(predicted_labels))\n",
    "            \n",
    "        regularization = 0.0\n",
    "        if lamda_regularizer > 0:\n",
    "            regularizer_1 = tf.contrib.layers.l2_regularizer(lamda_regularizer)\n",
    "            regularization = regularizer_1(\n",
    "                self.weights['embedding_users']) + regularizer_1(\n",
    "                self.weights['embedding_items'])+ regularizer_1(\n",
    "                self.weights['cnn_1']) + regularizer_1(\n",
    "                self.weights['linear'])+ regularizer_1(\n",
    "                self.weights['cnn_ui']) + regularizer_1(\n",
    "                self.weights['linear_0'])\n",
    "            if self.kernel_size == 2:\n",
    "                regularization = regularization + regularizer_1(self.weights['cnn_2'])\n",
    "        \n",
    "        cost = cost_uij + self.alpha * cost_ui + regularization\n",
    "        return cost    \n",
    "        \n",
    "        \n",
    "    def evaluate(self, test_sequence, topK=10):\n",
    "        score = np.zeros([self.users_num, self.items_num])\n",
    "        users = np.array([u for u in range(self.users_num)])\n",
    "        items = np.array([i for i in range(self.items_num)])\n",
    "  \n",
    "        for u in range(self.users_num):\n",
    "            user_ids = np.reshape(u * np.ones([self.items_num]),(-1,1))\n",
    "            feed_dict = {self.user_inputs: user_ids, self.item_inputs:np.c_[items,items]}\n",
    "            out = self.sess.run([self.predictions], feed_dict=feed_dict)\n",
    "            score[u,:] = np.reshape(out,(-1, self.items_num))\n",
    "            \n",
    "        ranklist = get_topk(prediction=score, test_sequence=np.array(test_sequence), topK=topK)\n",
    "        #print(ranklist)\n",
    "        hits,ndcgs = hit_ndcg(test_sequence=np.array(test_sequence), ranklist=ranklist)\n",
    "        hr,ndcg = np.array(hits).mean(),np.array(ndcgs).mean()\n",
    "        return hr,ndcg\n",
    "    \n",
    "    def predict(self, pairs, batch_size, verbose):\n",
    "        \"\"\"Computes predictions for a given set of user-item pairs.\n",
    "        Args:\n",
    "          pairs: A pair of lists (users, items) of the same length.\n",
    "          batch_size: unused.\n",
    "          verbose: unused.\n",
    "        Returns:\n",
    "          predictions: A list of the same length as users and items, such that\n",
    "          predictions[i] is the models prediction for (users[i], items[i]).\n",
    "        \"\"\"\n",
    "        del batch_size, verbose\n",
    "        num_examples = len(pairs[0])\n",
    "        assert num_examples == len(pairs[1])\n",
    "        predictions = np.empty(num_examples)\n",
    "        pairs = np.array(pairs, dtype=np.int16)\n",
    "        for i in range(num_examples):\n",
    "            feed_dict = {self.user_inputs:np.reshape(pairs[0][i], (-1,1)),\n",
    "                         self.item_inputs:np.c_[pairs[1][i], pairs[1][i]]}\n",
    "            out = self.sess.run([self.predictions], feed_dict=feed_dict)\n",
    "            predictions[i] = np.reshape(out,(-1))\n",
    "            #predictions[i] = self._predict_one(pairs[0][i], pairs[1][i])\n",
    "        return predictions\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        embeddings = self.sess.run(self.weights['embedding_items'])\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_mat, test_ratings, test_negatives, users_num, items_num, train_list=None, test_list=None,\n",
    "          positive_size=1, negative_time=4, epochs=128, topK=10, mode='ndcg'):\n",
    "    \n",
    "    if train_list!=None:\n",
    "        train_mat= sequence2mat(sequence=train_list, N=users_num, M=items_num) # train data : user-item matrix\n",
    "        is_sparse = False\n",
    "    \n",
    "    hr_list=[]\n",
    "    ndcg_list=[]\n",
    "    hr, ndcg = evaluate(model, test_ratings, test_negatives, K=topK)\n",
    "    #hr,ndcg = model.evaluate(test_sequence=test_list, topK=topK)\n",
    "    hr_list.append(hr)\n",
    "    ndcg_list.append(ndcg)\n",
    "    print('Init: HR = %.4f, NDCG = %.4f' %(hr, ndcg))\n",
    "    best_hr, best_ndcg = hr, ndcg\n",
    "    for epoch in range(epochs):\n",
    "        data_sequence = generate_instances(\n",
    "            train_mat, positive_size=positive_size, negative_time=negative_time, is_sparse=True)\n",
    "        loss_records = model.train(data_sequence=data_sequence)\n",
    "        # Evaluation\n",
    "        hr, ndcg = evaluate(model, test_ratings, test_negatives, K=topK)\n",
    "        #hr,ndcg = model.evaluate(test_sequence=test_list, topK=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "        print('epoch=%d, loss=%.4f, HR=%.4f, NDCG=%.4f' %(epoch,loss_records[-1],hr,ndcg))\n",
    "        \n",
    "        mlist = hr_list\n",
    "        if mode == 'ndcg':\n",
    "            mlist = ndcg_list\n",
    "        if (len(mlist) > 10) and (mlist[-2] < mlist[-3] > mlist[-1]):\n",
    "            best_hr, best_ndcg = hr_list[-3], ndcg_list[-3]\n",
    "            break\n",
    "        best_hr, best_ndcg = hr, ndcg          \n",
    "            \n",
    "    print(\"End. Best HR = %.4f, NDCG = %.4f. \" %(best_hr, best_ndcg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: #user=943, #item=1682, #train_pairs=99057, #test_pairs=943\n",
      "Init: HR = 0.1029, NDCG = 0.0400\n",
      "epoch=0, loss=0.1745, HR=0.5440, NDCG=0.3044\n",
      "epoch=1, loss=0.1667, HR=0.6607, NDCG=0.3766\n",
      "epoch=2, loss=0.1459, HR=0.6755, NDCG=0.3904\n",
      "epoch=3, loss=0.1344, HR=0.6734, NDCG=0.3902\n",
      "epoch=4, loss=0.1250, HR=0.6734, NDCG=0.3977\n",
      "epoch=5, loss=0.1358, HR=0.6935, NDCG=0.3947\n",
      "epoch=6, loss=0.1295, HR=0.6988, NDCG=0.4001\n",
      "epoch=7, loss=0.1412, HR=0.7063, NDCG=0.4137\n",
      "epoch=8, loss=0.1202, HR=0.6978, NDCG=0.4160\n",
      "epoch=9, loss=0.1273, HR=0.7105, NDCG=0.4192\n",
      "epoch=10, loss=0.1304, HR=0.6893, NDCG=0.4099\n",
      "epoch=11, loss=0.1197, HR=0.6861, NDCG=0.4061\n",
      "End. Best HR = 0.7105, NDCG = 0.4192. \n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'data/100k'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset(dataset_path)\n",
    "train_mat, test_ratings, test_negatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "print('Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' % (dataset.num_users, dataset.num_items, train_mat.nnz, len(test_ratings)))\n",
    "\n",
    "embedding_size = [32]\n",
    "out_channels = 64\n",
    "learning_rate = 5e-3 #学习率\n",
    "lamda_regularizer = [1e-6]\n",
    "alpha = 1.0\n",
    "kernel_size = 3\n",
    "\n",
    "positive_size = 1\n",
    "negative_time = 4\n",
    "epochs = 64\n",
    "batch_size = 1024 #batch大小\n",
    "topK = 10\n",
    "mode = 'hr'\n",
    "\n",
    "for e in embedding_size:\n",
    "    for l in lamda_regularizer:\n",
    "        #创建模型\n",
    "        model = Net(users_num = dataset.num_users,\n",
    "                    items_num = dataset.num_items,\n",
    "                    batch_size = batch_size,\n",
    "                    embedding_size = e,\n",
    "                    out_channels = out_channels,\n",
    "                    kernel_size = kernel_size,\n",
    "                    learning_rate = learning_rate,\n",
    "                    lamda_regularizer = l,\n",
    "                    alpha = alpha)\n",
    "\n",
    "        # Train and evaluate model\n",
    "        train(model=model,\n",
    "              train_mat=train_mat.tocsr(), \n",
    "              test_ratings=test_ratings, \n",
    "              test_negatives=test_negatives, \n",
    "              users_num=dataset.num_users, \n",
    "              items_num=dataset.num_items,\n",
    "              positive_size=positive_size,\n",
    "              negative_time=negative_time,\n",
    "              epochs=epochs,\n",
    "              topK=topK,\n",
    "              mode=mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
