{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "# Dataset and evaluation protocols reused from\n",
    "# https://github.com/hexiangnan/neural_collaborative_filtering\n",
    "from Dataset import Dataset\n",
    "from evaluate import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instances(train_mat, positive_size=2, negative_time=8, is_sparse=False):\n",
    "    data = []\n",
    "    users_num,items_num = train_mat.shape\n",
    "    \n",
    "    if is_sparse:\n",
    "        indptr = train_mat.indptr\n",
    "        indices = train_mat.indices\n",
    "    for u in range(users_num):\n",
    "        if is_sparse:\n",
    "            rated_items = indices[indptr[u]:indptr[u+1]] #用户u中有评分项的id\n",
    "        else:\n",
    "            rated_items = np.where(train_mat[u,:]>0)[0]\n",
    "        \n",
    "        for item0 in rated_items:\n",
    "            for item1 in np.random.choice(rated_items, size=positive_size):\n",
    "                data.append([u,item0,item1,1.])\n",
    "            for _ in range(positive_size*negative_time):\n",
    "                item1 = np.random.randint(items_num) # no matter item1 is positive or negtive\n",
    "                item2 = np.random.randint(items_num)\n",
    "                while item2 in rated_items:\n",
    "                    item2 = np.random.randint(items_num)\n",
    "                data.append([u,item2,item1,0.])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed) # CPU seed\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed) # GPU\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "\n",
    "def evaluate(model, test_ratings, test_negatives, K=10):\n",
    "    \"\"\"Helper that calls evaluate from the NCF libraries.\"\"\"\n",
    "    (hits, ndcgs) = evaluate_model(model, test_ratings, test_negatives, K=K, num_thread=1)\n",
    "    return np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "\n",
    "\n",
    "def get_similar_items(item_mat, idx, topk=5):\n",
    "    m,k = item_mat.shape\n",
    "    target_item = item_mat[idx,:]\n",
    "    target_mat = np.reshape(np.tile(target_item,m),(-1,k))\n",
    "    sim = [np.dot(target_mat[i], item_mat[i])/(np.linalg.norm(target_mat[i])*np.linalg.norm(item_mat[i])) \n",
    "           for i in range(m)] \n",
    "    sorted_items = np.argsort(-np.array(sim))\n",
    "    return sorted_items[:topk+1] # the most similar is itself\n",
    "\n",
    "def get_key(item_dict, value):\n",
    "    key = -1\n",
    "    for (k, v) in item_dict.items():\n",
    "        if v == value:\n",
    "            key = k\n",
    "    return key\n",
    "\n",
    "\n",
    "# read original records\n",
    "def get_item_dict(file_dir):\n",
    "    # output: \n",
    "    # N: the number of user;\n",
    "    # M: the number of item\n",
    "    # data: the list of rating information\n",
    "    user_ids_dict, rated_item_ids_dict = {},{}\n",
    "    N, M, u_idx, i_idx = 0,0,0,0 \n",
    "    data_rating = []\n",
    "    data_time = []\n",
    "    f = open(file_dir)\n",
    "    for line in f.readlines():\n",
    "        if '::' in line:\n",
    "            u, i, r = line.split('::')[:3]\n",
    "        elif ',' in line:\n",
    "            u, i, r = line.split(',')[:3]\n",
    "        else:\n",
    "            u, i, r = line.split()[:3]\n",
    "    \n",
    "        if u not in user_ids_dict:\n",
    "            user_ids_dict[u]=u_idx\n",
    "            u_idx+=1\n",
    "        if i not in rated_item_ids_dict:\n",
    "            rated_item_ids_dict[i]=i_idx\n",
    "            i_idx+=1\n",
    "        data_rating.append([user_ids_dict[u],rated_item_ids_dict[i],float(r)])\n",
    "    \n",
    "    f.close()\n",
    "    N = u_idx\n",
    "    M = i_idx\n",
    "\n",
    "    return rated_item_ids_dict\n",
    "\n",
    "\n",
    "def id_name(file_dir):\n",
    "    id_name_dict = {}\n",
    "    f = open(file_dir, 'r', encoding='latin-1')\n",
    "    for line in f.readlines():\n",
    "        movie_id, movie_name = line.split('|')[:2]\n",
    "        id_name_dict[int(movie_id)] = movie_name\n",
    "        \n",
    "    return id_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UInet(nn.Module):\n",
    "    def __init__(self, embedding_user, embedding_item, embedding_size=16, out_channels=64, kernel_size=2, stride=1, padding=0, n_class=1):\n",
    "        super(UInet, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_user, self.embedding_item = embedding_user, embedding_item\n",
    "        self.cnn = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(int(((self.embedding_size-kernel_size+2*padding)/stride+1)*out_channels), n_class)\n",
    " \n",
    "    def forward(self, x):\n",
    "        embed_users = self.embedding_user(x[:,0])\n",
    "        embed_items0 = self.embedding_item(x[:,1])\n",
    "        embed_items1 = self.embedding_item(x[:,2])\n",
    "        out = torch.cat([embed_users, embed_items0],1).reshape(-1, 1, 2, self.embedding_size)\n",
    "        out = self.cnn(out)          \n",
    "        out = self.relu(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.linear(out) \n",
    "        return out\n",
    "    \n",
    "    def predict(self, pairs, batch_size, verbose):\n",
    "        \"\"\"Computes predictions for a given set of user-item pairs.\n",
    "        Args:\n",
    "          pairs: A pair of lists (users, items) of the same length.\n",
    "          batch_size: unused.\n",
    "          verbose: unused.\n",
    "        Returns:\n",
    "          predictions: A list of the same length as users and items, such that\n",
    "          predictions[i] is the models prediction for (users[i], items[i]).\n",
    "        \"\"\"\n",
    "        del batch_size, verbose\n",
    "        num_examples = len(pairs[0])\n",
    "        assert num_examples == len(pairs[1])\n",
    "        predictions = np.empty(num_examples)\n",
    "        pairs = np.array(pairs, dtype=np.int16)\n",
    "        for i in range(num_examples):\n",
    "            x = np.c_[pairs[0][i],pairs[1][i],pairs[1][i]]\n",
    "            x = torch.from_numpy(x).long()\n",
    "            out = self.forward(x)\n",
    "            predictions[i] = out.reshape(-1).data.numpy()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UIInet(nn.Module):\n",
    "    def __init__(self, embedding_user, embedding_item, embedding_size=16, out_channels=64, kernel_size=2, stride=1, padding=0, n_class=1):\n",
    "        super(UIInet, self).__init__()\n",
    "        self.embedding_size, self.kernel_size = embedding_size, kernel_size\n",
    "        self.embedding_user, self.embedding_item = embedding_user, embedding_item\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.relu = nn.ReLU()\n",
    "        if self.kernel_size == 2:\n",
    "            input_size = (self.embedding_size - self.kernel_size + 2 * padding)/stride + 1\n",
    "            channel_num = out_channels\n",
    "        else:\n",
    "            input_size = self.embedding_size\n",
    "            channel_num = out_channels\n",
    "        self.linear = nn.Linear(int(((input_size - self.kernel_size + 2 * padding)/stride + 1) * channel_num), n_class)\n",
    " \n",
    "    def forward(self, x):\n",
    "        embed_users = self.embedding_user(x[:,0])\n",
    "        embed_items0 = self.embedding_item(x[:,1])\n",
    "        embed_items1 = self.embedding_item(x[:,2])\n",
    "        out = torch.cat([embed_items0, embed_users, embed_items1],1).reshape(-1, 1, 3, self.embedding_size)\n",
    "        out = self.cnn1(out)          \n",
    "        out = self.relu(out)\n",
    "        if self.kernel_size == 2: \n",
    "            out = self.cnn2(out)          \n",
    "            out = self.relu(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.linear(out) \n",
    "        return out\n",
    "    \n",
    "    def predict(self, pairs, batch_size, verbose):\n",
    "        \"\"\"Computes predictions for a given set of user-item pairs.\n",
    "        Args:\n",
    "          pairs: A pair of lists (users, items) of the same length.\n",
    "          batch_size: unused.\n",
    "          verbose: unused.\n",
    "        Returns:\n",
    "          predictions: A list of the same length as users and items, such that\n",
    "          predictions[i] is the models prediction for (users[i], items[i]).\n",
    "        \"\"\"\n",
    "        del batch_size, verbose\n",
    "        num_examples = len(pairs[0])\n",
    "        assert num_examples == len(pairs[1])\n",
    "        predictions = np.empty(num_examples)\n",
    "        pairs = np.array(pairs, dtype=np.int16)\n",
    "        for i in range(num_examples):\n",
    "            x = np.c_[pairs[0][i],pairs[1][i],pairs[1][i]]\n",
    "            x = torch.from_numpy(x).long()\n",
    "            out = self.forward(x)\n",
    "            predictions[i] = out.reshape(-1).data.numpy()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, users_num, items_num, embedding_size=16, out_channels=64, kernel_size=2, stride=1, padding=0, n_class=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_size, self.kernel_size, self.items_num, self.users_num = embedding_size, kernel_size, items_num, users_num\n",
    "        self.embedding_user  = nn.Embedding(self.users_num, self.embedding_size)\n",
    "        self.embedding_item = nn.Embedding(self.items_num, self.embedding_size)\n",
    "        #self.embedding_user  = nn.Embedding.from_pretrained(torch.nn.init.normal(tensor=torch.Tensor(self.users_num, self.embedding_size), mean=0, std=0.1))\n",
    "        #self.embedding_item = nn.Embedding.from_pretrained(torch.nn.init.normal(tensor=torch.Tensor(self.items_num, self.embedding_size), mean=0, std=0.1))\n",
    "\n",
    "        self.net_ui = UInet(embedding_user=self.embedding_user, \n",
    "                            embedding_item=self.embedding_item, \n",
    "                            embedding_size=self.embedding_size, \n",
    "                            out_channels=out_channels, \n",
    "                            kernel_size=2, \n",
    "                            stride=stride, \n",
    "                            padding=padding, \n",
    "                            n_class=n_class)\n",
    "        self.net_uii = UIInet(embedding_user=self.embedding_user, \n",
    "                              embedding_item=self.embedding_item, \n",
    "                              embedding_size=self.embedding_size, \n",
    "                              out_channels=out_channels, \n",
    "                              kernel_size=self.kernel_size, \n",
    "                              stride=stride, \n",
    "                              padding=padding, \n",
    "                              n_class=n_class)\n",
    " \n",
    "    def forward(self, x):\n",
    "        out1 = self.net_ui(x)          \n",
    "        out2 = self.net_uii(x) \n",
    "        return out1, out2\n",
    "    \n",
    "    def predict(self, pairs, batch_size, verbose):\n",
    "        \"\"\"Computes predictions for a given set of user-item pairs.\n",
    "        Args:\n",
    "          pairs: A pair of lists (users, items) of the same length.\n",
    "          batch_size: unused.\n",
    "          verbose: unused.\n",
    "        Returns:\n",
    "          predictions: A list of the same length as users and items, such that\n",
    "          predictions[i] is the models prediction for (users[i], items[i]).\n",
    "        \"\"\"\n",
    "        del batch_size, verbose\n",
    "        num_examples = len(pairs[0])\n",
    "        assert num_examples == len(pairs[1])\n",
    "        predictions = np.empty(num_examples)\n",
    "        pairs = np.array(pairs, dtype=np.int16)\n",
    "        for i in range(num_examples):\n",
    "            x = np.c_[pairs[0][i],pairs[1][i],pairs[1][i]]\n",
    "            x = torch.from_numpy(x).long()\n",
    "            out, _ = self.forward(x)\n",
    "            predictions[i] = out.reshape(-1).data.numpy()\n",
    "        return predictions\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        idx = torch.LongTensor([i for i in range(self.items_num)])\n",
    "        embeddings = self.embedding_item(idx)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_mat, test_ratings, test_negatives, users_num, items_num, train_list=None, test_list=None,\n",
    "          learning_rate = 1e-2, weight_decay=1e-6, alpha=1., positive_size=1, negative_time=4, epochs=64, \n",
    "          batch_size=1024, topK=10, mode='hr'):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    #criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if train_list!=None:\n",
    "        train_mat= sequence2mat(sequence=train_list, N=users_num, M=items_num) # train data : user-item matrix\n",
    "        is_sparse = False\n",
    "    \n",
    "    hr_list=[]\n",
    "    ndcg_list=[]\n",
    "    hr, ndcg = evaluate(model, test_ratings, test_negatives, K=topK)\n",
    "    embeddings = model.get_embeddings()\n",
    "    hr_list.append(hr)\n",
    "    ndcg_list.append(ndcg)\n",
    "    print('Init: HR = %.4f, NDCG = %.4f' %(hr, ndcg))\n",
    "    best_hr, best_ndcg = hr, ndcg\n",
    "    for epoch in range(epochs):\n",
    "        data_sequence = generate_instances(train_mat, positive_size=positive_size, negative_time=negative_time, is_sparse=True)\n",
    "        #data_sequence = read_list(\"output/\" + str(epoch) + \".txt\")\n",
    "        \n",
    "        train_size = len(data_sequence)\n",
    "        np.random.shuffle(data_sequence)\n",
    "        batch_size = batch_size\n",
    "        total_batch = math.ceil(train_size/batch_size)\n",
    "\n",
    "        for batch in range(total_batch):\n",
    "            start = (batch*batch_size)% train_size\n",
    "            end = min(start+batch_size, train_size)\n",
    "            data_array = np.array(data_sequence[start:end])\n",
    "            x = torch.from_numpy(data_array[:,:3]).long()\n",
    "            y = torch.from_numpy(data_array[:,-1]).reshape(-1,1)\n",
    "            y1, y2 = model(x)\n",
    "            loss = criterion(y2.float(), y.float()) + alpha * criterion(y1.float(), y.float())\n",
    "            optimizer.zero_grad()              # clear gradients for this training step\n",
    "            loss.backward()                    # backpropagation, compute gradients\n",
    "            optimizer.step()                   # apply gradients\n",
    "            \n",
    "        # Evaluation\n",
    "        hr, ndcg = evaluate(model, test_ratings, test_negatives, K=topK)\n",
    "        hr_list.append(hr)\n",
    "        ndcg_list.append(ndcg)\n",
    "        print('epoch=%d, loss=%.4f, HR=%.4f, NDCG=%.4f' %(epoch, loss, hr, ndcg))\n",
    "        \n",
    "        mlist = hr_list\n",
    "        if mode == 'ndcg':\n",
    "            mlist = ndcg_list\n",
    "        if (len(mlist) > 10) and (mlist[-2] < mlist[-3] > mlist[-1]):\n",
    "            best_hr, best_ndcg = hr_list[-3], ndcg_list[-3]\n",
    "            embeddings = model.get_embeddings()\n",
    "            break\n",
    "        best_hr, best_ndcg = hr, ndcg\n",
    "        embeddings = model.get_embeddings()\n",
    "            \n",
    "    print(\"End. Best HR = %.4f, NDCG = %.4f. \" %(best_hr, best_ndcg))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: #user=518, #item=3488, #train_pairs=45654, #test_pairs=518\n",
      "Init: HR = 0.1120, NDCG = 0.0532\n",
      "epoch=0, loss=0.2675, HR=0.4730, NDCG=0.2999\n",
      "epoch=1, loss=0.2381, HR=0.4788, NDCG=0.2959\n",
      "epoch=2, loss=0.2332, HR=0.4884, NDCG=0.3024\n",
      "epoch=3, loss=0.2064, HR=0.5695, NDCG=0.3722\n",
      "epoch=4, loss=0.1819, HR=0.6506, NDCG=0.4532\n",
      "epoch=5, loss=0.1630, HR=0.6776, NDCG=0.4723\n",
      "epoch=6, loss=0.1713, HR=0.6969, NDCG=0.4862\n",
      "epoch=7, loss=0.1696, HR=0.7162, NDCG=0.4921\n",
      "epoch=8, loss=0.1431, HR=0.7008, NDCG=0.5033\n",
      "epoch=9, loss=0.1468, HR=0.7317, NDCG=0.5302\n",
      "epoch=10, loss=0.1344, HR=0.7375, NDCG=0.5356\n",
      "epoch=11, loss=0.1298, HR=0.7375, NDCG=0.5191\n",
      "epoch=12, loss=0.1164, HR=0.7375, NDCG=0.5259\n",
      "epoch=13, loss=0.1344, HR=0.7490, NDCG=0.5323\n",
      "epoch=14, loss=0.1210, HR=0.7568, NDCG=0.5326\n",
      "epoch=15, loss=0.1179, HR=0.7529, NDCG=0.5406\n",
      "epoch=16, loss=0.1303, HR=0.7548, NDCG=0.5419\n",
      "epoch=17, loss=0.1192, HR=0.7606, NDCG=0.5313\n",
      "epoch=18, loss=0.1164, HR=0.7548, NDCG=0.5221\n",
      "epoch=19, loss=0.1206, HR=0.7568, NDCG=0.5383\n",
      "End. Best HR = 0.7606, NDCG = 0.5313. \n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'data/lastfm'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset(dataset_path)\n",
    "train_mat, test_ratings, test_negatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "print('Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' \n",
    "      % (dataset.num_users, dataset.num_items, train_mat.nnz, len(test_ratings)))\n",
    "\n",
    "embedding_size = 32# e=32,o=32 or 64\n",
    "out_channels = 8\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-6\n",
    "alpha = 1.0\n",
    "kernel_size = 2\n",
    "\n",
    "positive_size = 1\n",
    "negative_time = 4\n",
    "epochs = 64\n",
    "batch_size = 1024 #batch大小\n",
    "topK = 10\n",
    "mode = 'hr'\n",
    "\n",
    "setup_seed(3)\n",
    "# Initialize the model\n",
    "model = Net(users_num=dataset.num_users, items_num=dataset.num_items, \n",
    "            embedding_size=embedding_size, out_channels=out_channels, kernel_size=kernel_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Train and evaluate model\n",
    "embeddings = train(model=model, \n",
    "                  train_mat=train_mat.tocsr(), \n",
    "                  test_ratings=test_ratings, \n",
    "                  test_negatives=test_negatives, \n",
    "                  users_num=dataset.num_users, \n",
    "                  items_num=dataset.num_items,  \n",
    "                  learning_rate=learning_rate,\n",
    "                  weight_decay=weight_decay,\n",
    "                  alpha=alpha,\n",
    "                  positive_size=positive_size,\n",
    "                  negative_time=negative_time,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  topK=topK,\n",
    "                  mode=mode)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: #user=943, #item=1682, #train_pairs=99057, #test_pairs=943\n",
      "10\n",
      "Init: HR = 0.0954, NDCG = 0.0439\n",
      "epoch=0, loss=0.2044, HR=0.4698, NDCG=0.2583\n",
      "epoch=1, loss=0.2236, HR=0.4751, NDCG=0.2656\n",
      "epoch=2, loss=0.2082, HR=0.4719, NDCG=0.2660\n",
      "epoch=3, loss=0.1948, HR=0.4825, NDCG=0.2658\n",
      "epoch=4, loss=0.1928, HR=0.5716, NDCG=0.3157\n",
      "epoch=5, loss=0.1681, HR=0.6246, NDCG=0.3637\n",
      "epoch=6, loss=0.1427, HR=0.6543, NDCG=0.3731\n",
      "epoch=7, loss=0.1357, HR=0.6829, NDCG=0.3982\n",
      "epoch=8, loss=0.1260, HR=0.6967, NDCG=0.4124\n",
      "epoch=9, loss=0.1400, HR=0.6903, NDCG=0.4039\n",
      "epoch=10, loss=0.1313, HR=0.7063, NDCG=0.4203\n",
      "epoch=11, loss=0.1178, HR=0.7137, NDCG=0.4267\n",
      "epoch=12, loss=0.1345, HR=0.7031, NDCG=0.4067\n",
      "epoch=13, loss=0.1144, HR=0.7010, NDCG=0.4122\n",
      "End. Best HR = 0.7137, NDCG = 0.4267. \n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'data/100k'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset(dataset_path)\n",
    "train_mat, test_ratings, test_negatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "print('Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' \n",
    "      % (dataset.num_users, dataset.num_items, train_mat.nnz, len(test_ratings)))\n",
    "\n",
    "embedding_size = 32 # e=32,o=32 or 64 when kernel_size==3\n",
    "out_channels = 64\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-6\n",
    "alpha = 1.0\n",
    "kernel_size = 3\n",
    "\n",
    "positive_size = 1\n",
    "negative_time = 4\n",
    "epochs = 64\n",
    "batch_size = 1024 #batch大小\n",
    "topK = 10\n",
    "mode = 'hr'\n",
    "seed = 10\n",
    "\n",
    "setup_seed(seed)\n",
    "# Initialize the model\n",
    "model = Net(users_num=dataset.num_users, items_num=dataset.num_items, \n",
    "            embedding_size=embedding_size, out_channels=out_channels, kernel_size=kernel_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Train and evaluate model\n",
    "embeddings = train(model=model, \n",
    "                  train_mat=train_mat.tocsr(), \n",
    "                  test_ratings=test_ratings, \n",
    "                  test_negatives=test_negatives, \n",
    "                  users_num=dataset.num_users, \n",
    "                  items_num=dataset.num_items,  \n",
    "                  learning_rate=learning_rate,\n",
    "                  weight_decay=weight_decay,\n",
    "                  alpha=alpha,\n",
    "                  positive_size=positive_size,\n",
    "                  negative_time=negative_time,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  topK=topK,\n",
    "                  mode=mode)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: #user=943, #item=1682, #train_pairs=99057, #test_pairs=943\n",
      "Init: HR = 0.0838, NDCG = 0.0373\n",
      "epoch=0, loss=0.2003, HR=0.4719, NDCG=0.2534\n",
      "epoch=1, loss=0.2092, HR=0.4942, NDCG=0.2671\n",
      "epoch=2, loss=0.1691, HR=0.6151, NDCG=0.3450\n",
      "epoch=3, loss=0.1700, HR=0.6532, NDCG=0.3780\n",
      "epoch=4, loss=0.1415, HR=0.6776, NDCG=0.3911\n",
      "epoch=5, loss=0.1506, HR=0.6882, NDCG=0.4033\n",
      "epoch=6, loss=0.1276, HR=0.6914, NDCG=0.4099\n",
      "epoch=7, loss=0.1406, HR=0.7020, NDCG=0.4112\n",
      "epoch=8, loss=0.1369, HR=0.6999, NDCG=0.4137\n",
      "epoch=9, loss=0.1320, HR=0.7052, NDCG=0.4213\n",
      "epoch=10, loss=0.1236, HR=0.6935, NDCG=0.4077\n",
      "epoch=11, loss=0.1430, HR=0.6903, NDCG=0.4150\n",
      "End. Best HR = 0.7052, NDCG = 0.4213. \n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'data/100k'\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset(dataset_path)\n",
    "train_mat, test_ratings, test_negatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "print('Dataset: #user=%d, #item=%d, #train_pairs=%d, #test_pairs=%d' \n",
    "      % (dataset.num_users, dataset.num_items, train_mat.nnz, len(test_ratings)))\n",
    "\n",
    "embedding_size = 32 # e=32,o=8 when kernel_size==2\n",
    "out_channels = 16\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-6\n",
    "alpha = 1.0\n",
    "kernel_size = 2\n",
    "\n",
    "positive_size = 1\n",
    "negative_time = 4\n",
    "epochs = 64\n",
    "batch_size = 1024 #batch大小\n",
    "topK = 10\n",
    "mode = 'hr'\n",
    "seed = 17\n",
    "\n",
    "\n",
    "setup_seed(seed)\n",
    "# Initialize the model\n",
    "model = Net(users_num=dataset.num_users, items_num=dataset.num_items, \n",
    "            embedding_size=embedding_size, out_channels=out_channels, kernel_size=kernel_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Train and evaluate model\n",
    "embeddings = train(model=model, \n",
    "                  train_mat=train_mat.tocsr(), \n",
    "                  test_ratings=test_ratings, \n",
    "                  test_negatives=test_negatives, \n",
    "                  users_num=dataset.num_users, \n",
    "                  items_num=dataset.num_items,  \n",
    "                  learning_rate=learning_rate,\n",
    "                  weight_decay=weight_decay,\n",
    "                  alpha=alpha,\n",
    "                  positive_size=positive_size,\n",
    "                  negative_time=negative_time,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  topK=topK,\n",
    "                  mode=mode)\n",
    "print('----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build the dict: original id <-> new id, original id <-> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = 'datasets/ml-100k/u.item'\n",
    "id_name_dict = id_name(file_dir) # original id : movie name\n",
    "\n",
    "file_dir = 'datasets/ml-100k/u.data'\n",
    "item_dict = get_item_dict(file_dir) # original id : new id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieID: 174 ; MovieName: Raiders of the Lost Ark (1981)\n",
      "\n",
      "1: Empire Strikes Back, The (1980)\n",
      "\n",
      "2: Back to the Future (1985)\n",
      "\n",
      "3: Pulp Fiction (1994)\n",
      "\n",
      "4: Aliens (1986)\n",
      "\n",
      "5: Alien (1979)\n",
      "------------------------------------------------------------------\n",
      "MovieID: 195 ; MovieName: Terminator, The (1984)\n",
      "\n",
      "1: Terminator 2: Judgment Day (1991)\n",
      "\n",
      "2: Aliens (1986)\n",
      "\n",
      "3: Speed (1994)\n",
      "\n",
      "4: Empire Strikes Back, The (1980)\n",
      "\n",
      "5: Jurassic Park (1993)\n",
      "------------------------------------------------------------------\n",
      "MovieID: 449 ; MovieName: Star Trek: The Motion Picture (1979)\n",
      "\n",
      "1: Star Trek V: The Final Frontier (1989)\n",
      "\n",
      "2: Star Trek III: The Search for Spock (1984)\n",
      "\n",
      "3: Waterworld (1995)\n",
      "\n",
      "4: Star Trek VI: The Undiscovered Country (1991)\n",
      "\n",
      "5: Star Trek IV: The Voyage Home (1986)\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "movieid_list = [174, 195, 449]\n",
    "    \n",
    "for movieid in movieid_list:\n",
    "    print('MovieID:', movieid, '; MovieName:', id_name_dict[movieid])\n",
    "    original_id = str(movieid)\n",
    "    target_item = item_dict[original_id]\n",
    "\n",
    "    top5 = get_similar_items(embeddings.data.numpy(), idx=target_item)\n",
    "    movie_list = [get_key(item_dict=item_dict, value=i) for i in top5]\n",
    "    rec_list = [id_name_dict[int(movie_id)] for movie_id in movie_list[1:]]\n",
    "    for i in range(len(rec_list)):\n",
    "        print('\\n{0}: {1}'.format(i+1, rec_list[i]))\n",
    "    print('------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
